{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize periodic lightcurves\n",
    "\n",
    "This notebooks demonstrates how to visualize lightcurves for periodic objects in the Rubin commissioning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import lsdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "from io import StringIO\n",
    "from nested_pandas import NestedDtype\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable plotting warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rubin HATS catalogs\n",
    "\n",
    "- The commissioning data in HATS is served from the LSDB shared directory for commissioning, at USDF.\n",
    "- HATS (HEALPix Adaptive Tiling Scheme) is a HEALPix-based parquet format that enables fast parallelization of large scale workflows.\n",
    "- The data was imported and post-processed by the [DASH pipeline](../dash_doc/README.md). \n",
    "- It includes added-value columns for scientific use - e.g. PSF and science magnitudes, and timestamps for every source / forced source.\n",
    "- The latest data release available is that of weekly [2025_09](https://rubinobs.atlassian.net/browse/DM-49235)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp_release = \"w_2025_09\"\n",
    "hats_dir = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/hats\")\n",
    "hats_path = hats_dir / drp_release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Rubin data\n",
    "\n",
    "The relevant catalogs are `diaObject_lc` and `object_lc`. Each row corresponds to a single object with all its sources and nested sources embedded in separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_nested(df, columns):\n",
    "    return df.assign(\n",
    "        **{\n",
    "            col: df[col].astype(NestedDtype.from_pandas_arrow_dtype(df.dtypes[col]))\n",
    "            for col in columns\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_catalog = lsdb.read_hats(hats_path / \"diaObject_lc\")\n",
    "# We use the `cast_nested` utility method to cast columns into the NestedFrame type\n",
    "dia_catalog = dia_catalog.map_partitions(\n",
    "    cast_nested, columns=[\"diaSource\", \"diaForcedSource\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_catalog = lsdb.read_hats(hats_path / \"object_lc\")\n",
    "# We use the `cast_nested` utility method to cast columns into the NestedFrame type\n",
    "obj_catalog = obj_catalog.map_partitions(cast_nested, columns=[\"forcedSource\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the variable objects\n",
    "\n",
    "We have a selection of 10 variable objects, which were found from previous analysis of forced photometry on science images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T16:21:02.403956Z",
     "iopub.status.busy": "2025-02-17T16:21:02.403753Z",
     "iopub.status.idle": "2025-02-17T16:21:02.409459Z",
     "shell.execute_reply": "2025-02-17T16:21:02.408996Z",
     "shell.execute_reply.started": "2025-02-17T16:21:02.403940Z"
    }
   },
   "outputs": [],
   "source": [
    "variables_csv = \"\"\"ra,dec,period\n",
    "94.95546,-24.73952,0.12095\n",
    "95.30235,-25.27431,0.12248\n",
    "94.91626,-24.69648,0.12038\n",
    "95.12418,-25.04329,0.23554\n",
    "58.83506,-48.79122,0.56335\n",
    "94.92264,-25.23185,0.07672\n",
    "94.72086,-25.05767,0.17559\n",
    "94.97073,-25.13643,0.12048\n",
    "59.12997,-48.78522,0.11628\n",
    "94.72086,-25.05767,0.17554\n",
    "\"\"\"\n",
    "# We add the index as a column of the DataFrame so we can later retrieve\n",
    "# all the Rubin data from difference and science imaging\n",
    "variables_df = pd.read_csv(StringIO(variables_csv)).reset_index()\n",
    "# Transform the DataFrame into a LSDB Catalog\n",
    "variables_catalog = lsdb.from_dataframe(variables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossmatch the data\n",
    "\n",
    "Let's crossmatch the Rubin data with our desired variable objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XMATCH_RADIUS_ARCSEC = 0.2\n",
    "variable_object = variables_catalog.crossmatch(\n",
    "    obj_catalog, radius_arcsec=XMATCH_RADIUS_ARCSEC, suffixes=[\"_var\", \"\"]\n",
    ")\n",
    "variable_dia = variables_catalog.crossmatch(\n",
    "    dia_catalog, radius_arcsec=XMATCH_RADIUS_ARCSEC, suffixes=[\"_var\", \"\"]\n",
    ")\n",
    "# The result will have all the columns in obj_catalog suffixed with `_obj`,\n",
    "# all the columns in dia_catalog suffixed with `_dia`, as well as all the\n",
    "# columns in `variables_df` suffixed with `_var_obj`\n",
    "result = variable_object.join(\n",
    "    variable_dia, left_on=\"index_var\", right_on=\"index_var\", suffixes=[\"_obj\", \"_dia\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply filtering according to the quality flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"forcedSource_obj\", \"diaSource_dia\", \"diaForcedSource_dia\"]:\n",
    "    result = result.query(\n",
    "        f\"~{column}.psfFlux_flag\"\n",
    "        f\" and ~{column}.pixelFlags_saturated\"\n",
    "        f\" and ~{column}.pixelFlags_cr\"\n",
    "        f\" and ~{column}.pixelFlags_bad\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we instantiate a Dask Client to efficiently parallelize our computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers=20, threads_per_worker=1) as client:\n",
    "    # Sort light curves by variable object index for plotting purposes\n",
    "    result_df = result.compute().sort_values(\"index_var_obj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the lightcurves from the Rubin Data\n",
    "\n",
    "The following utility methods allow us to plot light curves from DIA source, DIA forced source and forcedSource, for each of our matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = {\n",
    "    \"u\": \"#56b4e9\",\n",
    "    \"g\": \"#009e73\",\n",
    "    \"r\": \"#f0e442\",\n",
    "    \"i\": \"#cc79a7\",\n",
    "    \"z\": \"#d55e00\",\n",
    "    \"y\": \"#0072b2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the magnitude scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mag_lightcurves(ax, row):\n",
    "    \"\"\"Plot magnitude light curves from DIA source, DIA forced source and forcedSource\"\"\"\n",
    "    # Define flux types for each dataset\n",
    "    datasets = [\n",
    "        (\"scienceFlux\", \"diaSourceTable_tract\", row.diaSource_dia),\n",
    "        (\"psfDiffFlux\", \"forcedSourceOnDiaObjectTable\", row.diaForcedSource_dia),\n",
    "        (\"psfFlux\", \"forcedSourceTable\", row.forcedSource_obj),\n",
    "    ]\n",
    "    all_mags = [[], []]  # To store magnitudes for each row\n",
    "    for i, (flux_col, table_name, lc) in enumerate(datasets):\n",
    "        title = f\"{flux_col} from {table_name}\"\n",
    "        flux_err_col = f\"{flux_col}Err\"\n",
    "        ax[0, i].set_title(title)\n",
    "        # Compute phase\n",
    "        lc = lc.assign(\n",
    "            phase=(lc.midpointMjdTai - lc.midpointMjdTai.loc[lc.psfFlux.idxmax()])\n",
    "            % row.period_var_obj\n",
    "            / row.period_var_obj\n",
    "        )\n",
    "        # First row: original light curve\n",
    "        all_mags[0].extend(\n",
    "            plot_mag_scale(\n",
    "                ax[0, i],\n",
    "                lc,\n",
    "                flux_col,\n",
    "                flux_err_col,\n",
    "                x_name=\"midpointMjdTai\",\n",
    "                x_label=\"MJD\",\n",
    "                show_legend=(i == 0),\n",
    "            )\n",
    "        )\n",
    "        # Second row: folded light curve\n",
    "        all_mags[1].extend(\n",
    "            plot_mag_scale(\n",
    "                ax[1, i], lc, flux_col, flux_err_col, x_name=\"phase\", x_label=\"Phase\"\n",
    "            )\n",
    "        )\n",
    "    return all_mags\n",
    "\n",
    "\n",
    "def plot_mag_scale(ax, lc, flux_col, flux_err_col, x_name, x_label, show_legend=False):\n",
    "    \"\"\"Plot light curves in magnitude scale\"\"\"\n",
    "    mag_values = []  # Store magnitudes for setting axis limits\n",
    "    for band, color in COLORS.items():\n",
    "        band_lc = lc.query(f\"band == '{band}'\")\n",
    "        # Compute magnitudes and errors\n",
    "        mag, magErr = create_mag_errors(band_lc[flux_col], band_lc[flux_err_col])\n",
    "        ax.errorbar(\n",
    "            band_lc[x_name],\n",
    "            mag,\n",
    "            magErr,\n",
    "            fmt=\"o\",\n",
    "            label=band,\n",
    "            color=color,\n",
    "            alpha=1,\n",
    "            markersize=5,\n",
    "            capsize=3,\n",
    "            elinewidth=1,\n",
    "        )\n",
    "        mag_values.extend(mag.dropna().values)  # Collect magnitude values\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Magnitude (AB)\")\n",
    "    ax.invert_yaxis()  # Magnitudes are plotted with brighter objects lower\n",
    "    if show_legend:\n",
    "        ax.legend(loc=\"lower right\", fontsize=12)  # Show legend in top-left panel only\n",
    "    return mag_values  # Return magnitudes for axis scaling\n",
    "\n",
    "\n",
    "def create_mag_errors(sciFlux, sciFluxErr):\n",
    "    \"\"\"Move flux into magnitudes and calculate the error on the magnitude\"\"\"\n",
    "    mag = u.nJy.to(u.ABmag, sciFlux)\n",
    "    upper_mag = u.nJy.to(u.ABmag, sciFlux + sciFluxErr)\n",
    "    lower_mag = u.nJy.to(u.ABmag, sciFlux - sciFluxErr)\n",
    "    magErr = -(upper_mag - lower_mag) / 2\n",
    "    return mag, magErr\n",
    "\n",
    "\n",
    "def scale_mag_y_axis(ax, all_mags):\n",
    "    \"\"\"Set uniform y-axis scaling for each plot row\"\"\"\n",
    "    for row_idx in range(2):\n",
    "        if all_mags[row_idx]:  # Ensure we have data\n",
    "            ymin, ymax = np.nanmin(all_mags[row_idx]), np.nanmax(all_mags[row_idx])\n",
    "            for i in range(3):  # Apply limits to all columns in the row\n",
    "                ax[row_idx, i].set_ylim(\n",
    "                    ymax + 0.1, ymin - 0.1\n",
    "                )  # Keep magnitude inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in result_df.iterrows():\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(16, 8), sharex=\"row\")  # 2 rows, 3 columns\n",
    "    fig.suptitle(\n",
    "        f\"{drp_release} | RA={row.ra_var_obj:.5f}, Dec={row.dec_var_obj:.5f}\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "    all_mags = plot_mag_lightcurves(ax, row)\n",
    "    scale_mag_y_axis(ax, all_mags)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the flux scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flux_lightcurves(ax, row):\n",
    "    \"\"\"Plot flux light curves from DIA source, DIA forced source and forcedSource\"\"\"\n",
    "    # Define flux types for each dataset\n",
    "    datasets = [\n",
    "        (\"scienceFlux\", \"diaSourceTable_tract\", row.diaSource_dia),\n",
    "        (\"psfDiffFlux\", \"forcedSourceOnDiaObjectTable\", row.diaForcedSource_dia),\n",
    "        (\"psfFlux\", \"forcedSourceTable\", row.forcedSource_obj),\n",
    "    ]\n",
    "    all_flux = [[], [], []]  # To store flux for each row\n",
    "    for i, (flux_col, table_name, lc) in enumerate(datasets):\n",
    "        title = f\"{flux_col} from {table_name}\"\n",
    "        flux_err_col = f\"{flux_col}Err\"\n",
    "        ax[0, i].set_title(title)\n",
    "        # Compute phase\n",
    "        lc = lc.assign(\n",
    "            phase=(lc.midpointMjdTai - lc.midpointMjdTai.loc[lc.psfFlux.idxmax()])\n",
    "            % row.period_var_obj\n",
    "            / row.period_var_obj\n",
    "        )\n",
    "        # First row: original light curve\n",
    "        all_flux[i].extend(\n",
    "            plot_flux_scale(\n",
    "                ax[0, i],\n",
    "                lc,\n",
    "                flux_col,\n",
    "                flux_err_col,\n",
    "                x_name=\"midpointMjdTai\",\n",
    "                x_label=\"MJD\",\n",
    "                show_legend=(i == 0),\n",
    "            )\n",
    "        )\n",
    "        # Second row: folded light curve\n",
    "        all_flux[i].extend(\n",
    "            plot_flux_scale(\n",
    "                ax[1, i], lc, flux_col, flux_err_col, x_name=\"phase\", x_label=\"Phase\"\n",
    "            )\n",
    "        )\n",
    "    return all_flux\n",
    "\n",
    "\n",
    "def plot_flux_scale(ax, lc, flux_col, flux_err_col, x_name, x_label, show_legend=False):\n",
    "    \"\"\"Function to plot light curves in flux scale\"\"\"\n",
    "    flux_values = []  # Store flux values for setting axis limits\n",
    "    for band, color in COLORS.items():\n",
    "        band_lc = lc.query(f\"band == '{band}'\")\n",
    "        # Extract flux values and errors directly\n",
    "        flux = band_lc[flux_col]\n",
    "        flux_err = band_lc[flux_err_col]\n",
    "        ax.errorbar(\n",
    "            band_lc[x_name],\n",
    "            flux,\n",
    "            flux_err,\n",
    "            fmt=\"o\",\n",
    "            label=band,\n",
    "            color=color,\n",
    "            alpha=1,\n",
    "            markersize=5,\n",
    "            capsize=3,\n",
    "            elinewidth=1,\n",
    "        )\n",
    "        flux_values.extend(flux.dropna().values)  # Collect flux values\n",
    "    ax.set_xlabel(x_label, fontsize=14)  # Increased font size\n",
    "    ax.set_ylabel(f\"{flux_col} (nJy)\", fontsize=14)  # Increased font size\n",
    "    if show_legend:\n",
    "        ax.legend(loc=\"lower right\", fontsize=12)  # Show legend in top-left panel only\n",
    "    return flux_values  # Return flux values for axis scaling\n",
    "\n",
    "\n",
    "def scale_flux_y_axis(ax, all_flux):\n",
    "    \"\"\"Set uniform y-axis scaling for first and third columns, while middle remains independent and symmetric\"\"\"\n",
    "    for row_idx in range(2):\n",
    "        for col_idx in [0, 2]:  # Apply shared limits to first and third columns\n",
    "            if all_flux[col_idx]:  # Ensure we have data\n",
    "                ymin, ymax = np.nanmin(all_flux[col_idx]), np.nanmax(all_flux[col_idx])\n",
    "                ax[row_idx, col_idx].set_ylim(\n",
    "                    ymin - 0.1 * abs(ymin), ymax + 0.1 * abs(ymax)\n",
    "                )  # Add buffer\n",
    "        # Middle column (psfDiffFlux) gets independent, symmetric scaling\n",
    "        if all_flux[1]:\n",
    "            max_abs_flux = np.nanmax(\n",
    "                np.abs(all_flux[1])\n",
    "            )  # Find the maximum absolute value\n",
    "            ax[row_idx, 1].set_ylim(\n",
    "                -max_abs_flux * 1.1, max_abs_flux * 1.1\n",
    "            )  # Symmetric range around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in result_df.iterrows():\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(16, 8), sharex=\"row\")  # 2 rows, 3 columns\n",
    "    fig.suptitle(\n",
    "        f\"{drp_release} | RA={row.ra_var_obj:.5f}, Dec={row.dec_var_obj:.5f}\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "    all_flux = plot_flux_lightcurves(ax, row)\n",
    "    scale_flux_y_axis(ax, all_flux)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
