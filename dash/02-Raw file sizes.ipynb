{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7248adc-f819-4435-a094-0c0e5f2802d1",
   "metadata": {},
   "source": [
    "# Raw file sizes\n",
    "\n",
    "Author: Melissa\n",
    "\n",
    "Explore the raw file sizes of the parquet files that back butler queries.\n",
    "\n",
    "This isn't super-useful, but I'm waiting for some slow import steps and I'm having fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c140e6e1-418a-420d-acce-451b4a9628da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:11:24.958832Z",
     "iopub.status.busy": "2025-01-29T23:11:24.958385Z",
     "iopub.status.idle": "2025-01-29T23:11:24.961637Z",
     "shell.execute_reply": "2025-01-29T23:11:24.961308Z",
     "shell.execute_reply.started": "2025-01-29T23:11:24.958810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generic python packages\n",
    "import os\n",
    "import pylab as plt\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.afw.display as afwDisplay\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set a standard figure size to use\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "afwDisplay.setDefaultBackend('matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b470d081-69f0-4cd5-9c68-6554c9325bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:11:25.693411Z",
     "iopub.status.busy": "2025-01-29T23:11:25.693004Z",
     "iopub.status.idle": "2025-01-29T23:11:25.695432Z",
     "shell.execute_reply": "2025-01-29T23:11:25.695113Z",
     "shell.execute_reply.started": "2025-01-29T23:11:25.693395Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dir = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/hats/w_2025_04/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6042aceb-4055-4934-b563-948209f94cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:11:26.406680Z",
     "iopub.status.busy": "2025-01-29T23:11:26.406257Z",
     "iopub.status.idle": "2025-01-29T23:11:26.410576Z",
     "shell.execute_reply": "2025-01-29T23:11:26.410191Z",
     "shell.execute_reply.started": "2025-01-29T23:11:26.406665Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_sizes(dataset_type):\n",
    "    file_pointer = raw_dir / \"paths\" / f\"{dataset_type}.txt\"\n",
    "    \n",
    "    with file_pointer.open(\"r\", encoding=\"utf8\") as _text_file:\n",
    "        paths = _text_file.readlines()\n",
    "    paths = [path.strip() for path in paths]\n",
    "\n",
    "    print(f\"Found {len(paths)} files for {dataset_type}\")\n",
    "\n",
    "    ref_frame = pd.read_csv(raw_dir / \"refs\" / f\"{dataset_type}.csv\") \n",
    "    ref_frame[\"paths\"] = paths\n",
    "    \n",
    "    num_columns = []\n",
    "    num_rows = []\n",
    "    num_row_groups = []\n",
    "    file_size = []\n",
    "    \n",
    "    for path in tqdm(paths):\n",
    "        parquet_md = pq.ParquetFile(path.strip()).metadata\n",
    "        num_columns.append(parquet_md.num_columns)\n",
    "        num_rows.append(parquet_md.num_rows)\n",
    "        num_row_groups.append(parquet_md.num_row_groups)\n",
    "        file_size.append(os.path.getsize(path))\n",
    "    \n",
    "    ref_frame[\"num_columns\"] = num_columns\n",
    "    ref_frame[\"num_rows\"] = num_rows\n",
    "    ref_frame[\"num_row_groups\"] = num_row_groups\n",
    "    ref_frame[\"file_size\"] = file_size\n",
    "    ref_frame[\"gbs\"] = ref_frame[\"file_size\"] / (1024 * 1024 * 1024)\n",
    "    \n",
    "    ref_frame.to_csv(raw_dir / \"sizes\" / f\"{dataset_type}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140819ad-e36d-4c78-8d10-f894eb1af545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:11:27.604425Z",
     "iopub.status.busy": "2025-01-29T23:11:27.604028Z",
     "iopub.status.idle": "2025-01-29T23:11:27.606516Z",
     "shell.execute_reply": "2025-01-29T23:11:27.606176Z",
     "shell.execute_reply.started": "2025-01-29T23:11:27.604410Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_types = [\n",
    "    'diaObjectTable_tract',\n",
    "    'diaSourceTable_tract',\n",
    "    'forcedSourceOnDiaObjectTable',\n",
    "    'objectTable',\n",
    "    'sourceTable',\n",
    "    'forcedSourceTable'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec92ee27-ea54-40cf-bb2e-8d80596d8f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:11:29.752967Z",
     "iopub.status.busy": "2025-01-29T23:11:29.752527Z",
     "iopub.status.idle": "2025-01-29T23:11:29.755038Z",
     "shell.execute_reply": "2025-01-29T23:11:29.754696Z",
     "shell.execute_reply.started": "2025-01-29T23:11:29.752951Z"
    }
   },
   "outputs": [],
   "source": [
    "for set_type in dataset_types:\n",
    "    get_all_sizes(set_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b057046-d568-4078-abbb-4887550617bd",
   "metadata": {},
   "source": [
    "## Estimate the pixel thresholds.\n",
    "\n",
    "Using something similar to [this old notebook](https://hats-import.readthedocs.io/en/latest/notebooks/estimate_pixel_threshold.html), but using the full dataset size and row count, we can get a pretty good idea of what good pixel thresholds are for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbbec98-0694-4786-b505-5ebcb002b976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:23:51.352093Z",
     "iopub.status.busy": "2025-01-29T23:23:51.351612Z",
     "iopub.status.idle": "2025-01-29T23:23:51.355582Z",
     "shell.execute_reply": "2025-01-29T23:23:51.355146Z",
     "shell.execute_reply.started": "2025-01-29T23:23:51.352075Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_import_stats(dataset_type):\n",
    "    all_sizes = pd.read_csv(raw_dir / \"sizes\" / f\"{dataset_type}.csv\")\n",
    "    sample_file_size=all_sizes[\"file_size\"].sum()\n",
    "    num_rows=all_sizes[\"num_rows\"].sum()\n",
    "    \n",
    "    ## 300MB\n",
    "    ideal_file_small = 300 * 1024 * 1024\n",
    "    ## 1G\n",
    "    ideal_file_large = 1024 * 1024 * 1024\n",
    "    \n",
    "    threshold_small = ideal_file_small / sample_file_size * num_rows\n",
    "    threshold_large = ideal_file_large / sample_file_size * num_rows\n",
    "\n",
    "    print(dataset_type)\n",
    "    print(f\"  threshold between {int(threshold_small):_} and {int(threshold_large):_}\")\n",
    "    print(f'  total size_on_disk: {all_sizes[\"gbs\"].sum():.2f} G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5412b1b9-054b-40cf-afc3-e1d89ea53519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:23:52.039130Z",
     "iopub.status.busy": "2025-01-29T23:23:52.038606Z",
     "iopub.status.idle": "2025-01-29T23:23:52.118249Z",
     "shell.execute_reply": "2025-01-29T23:23:52.117798Z",
     "shell.execute_reply.started": "2025-01-29T23:23:52.039114Z"
    }
   },
   "outputs": [],
   "source": [
    "for set_type in dataset_types:\n",
    "    print_import_stats(set_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
