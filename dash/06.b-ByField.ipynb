{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5060a38-77e8-44d6-8e0d-877edaa7268b",
   "metadata": {},
   "source": [
    "# Catalog inspection - By Field\n",
    "\n",
    "Perform more detailed verification on the datasets, using LSDB to inspect leaf parquet files, using spatial fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94284f4e-e2f1-40b5-b7e5-4afdd1148092",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lsdb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd84a72-fb12-4466-8bba-564a5565db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DRP_VERSION=w_2025_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e490de7-923f-4870-83b1-302075eabfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hats\n",
    "import lsdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02c256-f540-462d-90ce-50e1f02f193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRP_VERSION = os.environ[\"DRP_VERSION\"]\n",
    "print(f\"DRP_VERSION: {DRP_VERSION}\")\n",
    "base_output_dir = Path(f\"/sdf/data/rubin/shared/lsdb_commissioning\")\n",
    "raw_dir = base_output_dir / \"raw\" / DRP_VERSION\n",
    "hats_dir = base_output_dir / \"hats\" / DRP_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_dir = raw_dir / \"field_sizes\"\n",
    "fields_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19828-d3f3-4c9d-802d-b5a1c53d34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogs = [\n",
    "    \"diaObject\",\n",
    "    \"diaSource\",\n",
    "    \"diaForcedSource\",\n",
    "    \"object\",\n",
    "    \"source\",\n",
    "    \"forcedSource\",\n",
    "]   \n",
    "\n",
    "# Define the six fields with their coordinates\n",
    "fields = {\n",
    "    \"ECDFS\": (53.13, -28.10),  # Extended Chandra Deep Field South\n",
    "    \"EDFS\": (59.10, -48.73),  # Euclid Deep Field South\n",
    "    \"Rubin_SV_38_7\": (37.86, 6.98),  # Low Ecliptic Latitude Field\n",
    "    \"Rubin_SV_95_-25\": (95.00, -25.00),  # Low Galactic Latitude Field\n",
    "    \"47_Tuc\": (6.02, -72.08),  # 47 Tuc Globular Cluster\n",
    "    \"Fornax_dSph\": (40.00, -34.45)  # Fornax Dwarf Spheroidal Galaxy\n",
    "}\n",
    "\n",
    "# Define the radius for selecting sources\n",
    "selection_radius_arcsec = 2.0 * 3600  # 2-degree radius\n",
    "\n",
    "# Define bands\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be6ff6-9c43-46d6-95d5-0a7ebf8ef1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = lsdb.read_hats(hats_dir / \"source\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01d253-524f-47a5-ab95-4f5533375d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What are the columns of interest for the results? Everything numeric!\n",
    "print(\"starting column count\", len(cat._ddf.meta.columns))\n",
    "column_names = list(cat._ddf.meta.select_dtypes(include=np.number))\n",
    "# Exclude HATS-added columns\n",
    "column_names = [c for c in column_names if c not in [\"_healpix_29\", \"Norder\", \"Dir\", \"Npix\"]]\n",
    "column_names = [c for c in column_names if not c.endswith(\"Id\")]\n",
    "column_names = [c for c in column_names if \"Mag\" not in c]\n",
    "print(\"effective column count\", len(column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute statistics\n",
    "def get_stats(df, stat_columns, out_columns):\n",
    "    stats={col: 0 for col in out_columns} \n",
    "    for band in bands:\n",
    "        mask = df[\"band\"] == band  # Filter by band\n",
    "        if mask.sum() > 0:  # Ensure there are sources in this band\n",
    "            for col in stat_columns:\n",
    "                stats[f\"mean_{col}_{band}\"] = np.nanmean(df.loc[mask,col])\n",
    "            stats[f\"len_{band}\"] = len(df.loc[mask, \"x\"]) - np.count_nonzero(np.isnan(df.loc[mask, \"x\"]))\n",
    "    return pd.DataFrame([stats])  # Convert to DataFrame\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "meta={**{f\"mean_{column}_{band}\": \"f8\" for (column, band) in itertools.product(column_names, bands)},\n",
    "     **{f\"len_{band}\": \"i8\" for band in bands}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfa0d8-bda1-4886-a834-460bb502caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each field and perform cone search + computation\n",
    "for field_name, (ra, dec) in tqdm(fields.items()):\n",
    "    # Perform cone search for the given field\n",
    "    field_cat = cat.cone_search(ra=ra, dec=dec, radius_arcsec=selection_radius_arcsec)\n",
    "    \n",
    "    # Compute statistics\n",
    "    result = field_cat.map_partitions(\n",
    "        get_stats,\n",
    "        meta=meta,\n",
    "        stat_columns=column_names,\n",
    "        out_columns=meta.keys(),\n",
    "    ).compute()\n",
    "\n",
    "    # Compute weighted sum for each band separately\n",
    "    weighted_means = {}\n",
    "    for (column, band) in itertools.product(column_names, bands):\n",
    "        if np.nansum(result[f\"len_{band}\"]):\n",
    "            mean_col_name = f\"mean_{column}_{band}\"\n",
    "            weighted_means[mean_col_name] = np.nansum(result[mean_col_name] * result[f\"len_{band}\"]) / np.nansum(result[f\"len_{band}\"])\n",
    "    \n",
    "    # Store the weighted means for this field\n",
    "    all_results[field_name] = weighted_means\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "weighted_means_df = pd.DataFrame.from_dict(all_results, orient=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e198eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "weighted_means_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3aee99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
