{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5a49bb-9251-4b4a-b347-fbef01b945eb",
   "metadata": {},
   "source": [
    "# DASH on a large instance\n",
    "\n",
    "Execution for [2025_04](https://rubinobs.atlassian.net/browse/DM-48556)\n",
    "\n",
    "This notebook uses the butler only to fetch the tracts/patches, and to fetch the URIs of backing parquet files. Those files are read into the hats-import pipeline directly.\n",
    "\n",
    "This is done because many `butler.get` results are too large to fit in the memory of a medium or large RSP notebook instance.\n",
    "\n",
    "Beyond the butler issues, there were additional problems with running the importer on a smaller instance. While these can largely be avoided by running on the dev machines that are available outside notebooks, I think it's a good lesson for how the Rubin data is structured and how we can more efficiently import with our existing tools.\n",
    "\n",
    "Useful material:\n",
    "- LINCC notebooks: https://github.com/lsst-sitcom/linccf\n",
    "- https://github.com/LSSTScienceCollaborations/StackClub/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3a6b07-f916-4e26-8862-ed50f2e669d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:28:36.544536Z",
     "iopub.status.busy": "2025-01-30T14:28:36.544039Z",
     "iopub.status.idle": "2025-01-30T14:28:36.546721Z",
     "shell.execute_reply": "2025-01-30T14:28:36.546389Z",
     "shell.execute_reply.started": "2025-01-30T14:28:36.544519Z"
    }
   },
   "outputs": [],
   "source": [
    "### UPDATE THIS CELL\n",
    "## Then run all cells.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "base_output_dir = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/dm_48556\")\n",
    "collections = 'LSSTComCam/runs/DRP/DP1/w_2025_04/DM-48556'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fda8024-983e-44e7-8c06-58159498f200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:28:37.876082Z",
     "iopub.status.busy": "2025-01-30T14:28:37.875831Z",
     "iopub.status.idle": "2025-01-30T14:28:37.878403Z",
     "shell.execute_reply": "2025-01-30T14:28:37.878038Z",
     "shell.execute_reply.started": "2025-01-30T14:28:37.876067Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -q lsdb hats-import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab38dd4-ec6d-4d45-9b0d-f41652e9779b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:28:39.237735Z",
     "iopub.status.busy": "2025-01-30T14:28:39.237377Z",
     "iopub.status.idle": "2025-01-30T14:28:42.245682Z",
     "shell.execute_reply": "2025-01-30T14:28:42.245225Z",
     "shell.execute_reply.started": "2025-01-30T14:28:39.237719Z"
    }
   },
   "outputs": [],
   "source": [
    "# HATS/LSDB\n",
    "import lsdb\n",
    "import hats_import.pipeline as runner\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from rubin_reader import RubinParquetReader\n",
    "from dask.distributed import Client\n",
    "import tempfile\n",
    "from hats_import.catalog.file_readers import ParquetPyarrowReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d860e7b9-0230-41da-9d5f-aebcb925bb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:28:45.239901Z",
     "iopub.status.busy": "2025-01-30T14:28:45.239652Z",
     "iopub.status.idle": "2025-01-30T14:28:45.243165Z",
     "shell.execute_reply": "2025-01-30T14:28:45.242792Z",
     "shell.execute_reply.started": "2025-01-30T14:28:45.239887Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dir = base_output_dir / \"raw\"\n",
    "hats_dir = base_output_dir /  \"hats\"\n",
    "\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "hats_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1cc55-0705-4109-a2b0-cc51cb6d848f",
   "metadata": {},
   "source": [
    "### Import data to HATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6a6cf2-991d-4dca-b5e0-5155a960712a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:28:52.409598Z",
     "iopub.status.busy": "2025-01-30T14:28:52.409307Z",
     "iopub.status.idle": "2025-01-30T14:28:52.412334Z",
     "shell.execute_reply": "2025-01-30T14:28:52.412018Z",
     "shell.execute_reply.started": "2025-01-30T14:28:52.409583Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_paths(dataset_type, out_dir):\n",
    "    file_pointer = out_dir /\"paths\"/  f\"{dataset_type}.txt\"\n",
    "    with file_pointer.open(\"r\", encoding=\"utf8\") as _text_file:\n",
    "        paths = _text_file.readlines()\n",
    "\n",
    "    paths = [path.strip() for path in paths]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae0f42",
   "metadata": {},
   "source": [
    "#### DiaObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c9060-d650-4f51-b43a-264aee21c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"diaObjectTable_tract\"\n",
    "\n",
    "diaObj_default_columns = [\"diaObjectId\", \"ra\", \"dec\", \"nDiaSources\", \"radecMjdTai\"]\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"diaObject\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(column_names=diaObj_default_columns),\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    catalog_type=\"object\",\n",
    "    resume=False,\n",
    "    pixel_threshold=2_000_000,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb1d3f",
   "metadata": {},
   "source": [
    "#### DiaSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d909565-2e74-4f7c-9ca4-6dd7f7abd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"diaSourceTable_tract\"\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"diaSource\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(),\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    catalog_type=\"source\",\n",
    "    resume=False,\n",
    "    pixel_threshold=2_000_000,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088c7aa",
   "metadata": {},
   "source": [
    "#### DiaForcedSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1a876-2466-4c8c-9b06-f24ea4e4eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"forcedSourceOnDiaObjectTable\"\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"diaForcedSource\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(),\n",
    "    ra_column=\"coord_ra\",\n",
    "    dec_column=\"coord_dec\",\n",
    "    catalog_type=\"source\",\n",
    "    pixel_threshold=5_000_000,\n",
    "    highest_healpix_order=12,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ba98d",
   "metadata": {},
   "source": [
    "#### Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46895e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_per_band = []\n",
    "for band in list(\"ugrizy\"):\n",
    "    for flux_type in [\"psf\",\"kron\"]:\n",
    "        prefix = f\"{band}_{flux_type}\"\n",
    "        cols_per_band.extend([f\"{prefix}Flux\", f\"{prefix}FluxErr\"])\n",
    "    cols_per_band.append(f\"{band}_kronRad\")\n",
    "    \n",
    "obj_default_columns = [\n",
    "    \"objectId\",\n",
    "    \"refFwhm\",\n",
    "    \"shape_flag\",\n",
    "    \"sky_object\",\n",
    "    \"parentObjectId\",\n",
    "    \"detect_isPrimary\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"xErr\",\n",
    "    \"yErr\",\n",
    "    \"shape_yy\", \n",
    "    \"shape_xx\", \n",
    "    \"shape_xy\", \n",
    "    \"coord_ra\",\n",
    "    \"coord_dec\", \n",
    "    \"coord_raErr\", \n",
    "    \"coord_decErr\",\n",
    "    \"tract\",\n",
    "    \"patch\",\n",
    "    \"detect_isIsolated\"\n",
    "] + cols_per_band\n",
    "\n",
    "obj_default_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4292007-30bd-4822-991c-300e0a3ff017",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"objectTable\"\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"object\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(column_names=obj_default_columns),\n",
    "    ra_column=\"coord_ra\",\n",
    "    dec_column=\"coord_dec\",\n",
    "    catalog_type=\"object\",\n",
    "    resume=False,\n",
    "    pixel_threshold=300_000,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bb848",
   "metadata": {},
   "source": [
    "#### Source\n",
    "\n",
    "This is one that's going to get much worse very quickly. The `sourceTable` dimension is on the visit. So each file is very small, and there are LOTS of them. \n",
    "\n",
    "```\n",
    "Planning  : 100% 4/4 [00:00<00:00, 123.68it/s]\n",
    "Mapping   : 100% 16471/16471 [04:25<00:00,  1.77it/s]\n",
    "Binning   : 100% 2/2 [00:38<00:00, 17.09s/it]\n",
    "Splitting : 100% 16471/16471 [28:41<00:00,  1.64s/it]\n",
    "Reducing  : 100% 148/148 [04:30<00:00,  2.21s/it]\n",
    "Finishing : 100% 5/5 [00:24<00:00,  8.99s/it]\n",
    "```\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- Use the `IndexedParquetReader`. We can aggregate each index file by something like tract/patch of the visit, to reduce intermediate file usage.\n",
    "- Escalate to DM. This is going to be ROUGH for everyone if there is no aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3ef3e-e426-4282-92e8-897bd11c87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"sourceTable\"\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"source\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(),\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    catalog_type=\"source\",\n",
    "    resume=False,\n",
    "    pixel_threshold=1_000_000,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e867bf",
   "metadata": {},
   "source": [
    "#### ForcedSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8864b-4af7-4a57-b87e-07a9cecdc673",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = download_visits(raw_dir)\n",
    "visit_map = visits[[\"expMidptMJD\"]].T.to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7519309d-509c-4074-965f-aef6e239ac5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:29:01.831212Z",
     "iopub.status.busy": "2025-01-30T14:29:01.830919Z",
     "iopub.status.idle": "2025-01-30T14:37:46.203302Z",
     "shell.execute_reply": "2025-01-30T14:37:46.202939Z",
     "shell.execute_reply.started": "2025-01-30T14:29:01.831192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c0c9b23e74ec5b55b5a6ea2208598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planning  :   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7ff6b8f3c7449b98ebbbf4938f30a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping   :   0%|          | 0/605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e5b6b7d7854521b53a4b02f9cf2bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binning   :   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8faff5f1a02943cabe884353ab8b88c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting :   0%|          | 0/605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78353665b25a4215ba434877aea85774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reducing  :   0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 14:35:12,959 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:35:21,760 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:35:41,566 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:35:42,782 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:35:43,262 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 3.16 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:36:06,163 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:36:06,662 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 3.17 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:36:36,271 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:37:39,362 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 4.00 GiB\n",
      "2025-01-30 14:37:39,555 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 3.19 GiB -- Worker memory limit: 4.00 GiB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4ab3f179cc4e3cadecaca2bd4e8328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finishing :   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"forcedSourceTable\"\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_path=hats_dir,\n",
    "    output_artifact_name=\"forcedSource\",\n",
    "    input_file_list=get_paths(dataset_type, raw_dir),\n",
    "    file_reader=ParquetPyarrowReader(),\n",
    "    ra_column=\"coord_ra\",\n",
    "    dec_column=\"coord_dec\",\n",
    "    catalog_type=\"source\",\n",
    "    resume=False,\n",
    "    pixel_threshold=8_000_000,\n",
    ")\n",
    "runner.pipeline_with_client(args, client)\n",
    "\n",
    "# Planning  : 100% 4/4 [00:00<00:00, 352.13it/s]\n",
    "# Mapping   : 100% 605/605 [00:18<00:00, 26.07it/s]\n",
    "# Binning   : 100% 2/2 [00:08<00:00,  4.85s/it]\n",
    "# Splitting : 100% 605/605 [04:59<00:00,  1.87s/it]\n",
    "# Reducing  : 100% 207/207 [03:15<00:00,  1.01s/it]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
