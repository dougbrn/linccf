{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5060a38-77e8-44d6-8e0d-877edaa7268b",
   "metadata": {},
   "source": [
    "# Catalog inspection\n",
    "\n",
    "I want to look at the \"fresh\" datasets (that have come directly from butler), and will also look at the post-processed DASH datasets (once I've generated them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e490de7-923f-4870-83b1-302075eabfb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:29:37.636251Z",
     "iopub.status.busy": "2025-01-30T16:29:37.636012Z",
     "iopub.status.idle": "2025-01-30T16:29:39.511734Z",
     "shell.execute_reply": "2025-01-30T16:29:39.511309Z",
     "shell.execute_reply.started": "2025-01-30T16:29:37.636235Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# HATS/LSDB\n",
    "import lsdb\n",
    "import hats\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb02c256-f540-462d-90ce-50e1f02f193b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:29:40.491018Z",
     "iopub.status.busy": "2025-01-30T16:29:40.490470Z",
     "iopub.status.idle": "2025-01-30T16:29:40.493475Z",
     "shell.execute_reply": "2025-01-30T16:29:40.493104Z",
     "shell.execute_reply.started": "2025-01-30T16:29:40.491001Z"
    }
   },
   "outputs": [],
   "source": [
    "base_output_dir = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/dm_48556\")\n",
    "hats_dir = base_output_dir /  \"hats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc19828-d3f3-4c9d-802d-b5a1c53d34a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:47:19.876780Z",
     "iopub.status.busy": "2025-01-30T16:47:19.876454Z",
     "iopub.status.idle": "2025-01-30T16:47:23.558146Z",
     "shell.execute_reply": "2025-01-30T16:47:23.557660Z",
     "shell.execute_reply.started": "2025-01-30T16:47:19.876757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diaForcedSource\n",
      "num partitions 510\n",
      "num rows 1065086776\n",
      "diaObject\n",
      "num partitions 6\n",
      "num rows 4064285\n",
      "diaSource\n",
      "num partitions 12\n",
      "num rows 6922037\n",
      "forcedSource\n",
      "num partitions 207\n",
      "num rows 592040780\n",
      "object\n",
      "num partitions 54\n",
      "num rows 5448838\n",
      "source\n",
      "num partitions 148\n",
      "num rows 54067391\n"
     ]
    }
   ],
   "source": [
    "fresh_catalogs = [\"diaForcedSource\", \"diaObject\", \"diaSource\", \"forcedSource\", \"object\", \"source\"]\n",
    "\n",
    "for catalog in fresh_catalogs:\n",
    "    cat = hats.read_hats(hats_dir / catalog)\n",
    "    print(catalog)\n",
    "    print(\"num partitions\", len(cat.get_healpix_pixels()))\n",
    "    print(\"num rows\", cat.catalog_info.total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ef7e1aa-8685-4e08-a74a-d9e571622ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:04:41.155395Z",
     "iopub.status.busy": "2025-01-30T17:04:41.155063Z",
     "iopub.status.idle": "2025-01-30T17:04:41.214496Z",
     "shell.execute_reply": "2025-01-30T17:04:41.214066Z",
     "shell.execute_reply.started": "2025-01-30T17:04:41.155380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile(hats_dir / \"source\" / \"dataset\" / \"_metadata\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = parquet_file.metadata.num_columns\n",
    "num_row_groups = parquet_file.metadata.num_row_groups\n",
    "sizes = np.zeros(num_cols)\n",
    "\n",
    "for rg in range(num_row_groups):\n",
    "    for col in range(num_cols):\n",
    "        sizes[col] += parquet_file.metadata.row_group(rg).column(col).total_compressed_size\n",
    "\n",
    "## This is just an attempt at pretty formatting\n",
    "percents = [f\"{s/sizes.sum()*100:.1f}\" for s in sizes]\n",
    "pd.DataFrame({\"name\": parquet_file.schema.names, \"size\": sizes.astype(int), \"percent\": percents}).sort_values(\n",
    "    \"size\", ascending=False\n",
    ").to_csv(\"/sdf/data/rubin/shared/lsdb_commissioning/dm_48556/raw/field_sizes/source.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95a35d-9b8d-4fc7-a73c-586cb58f8b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
