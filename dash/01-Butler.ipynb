{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5a49bb-9251-4b4a-b347-fbef01b945eb",
   "metadata": {},
   "source": [
    "# Fetch data from butler\n",
    "\n",
    "Execution for [2025_04](https://rubinobs.atlassian.net/browse/DM-48556)\n",
    "\n",
    "This notebook uses the butler only to fetch the tracts/patches, and to fetch the URIs of backing parquet files. Those files are read into the hats-import pipeline directly.\n",
    "\n",
    "This is done because many `butler.get` results are too large to fit in the memory of a medium or large RSP notebook instance.\n",
    "\n",
    "Beyond the butler issues, there were additional problems with running the importer on a smaller instance. While these can largely be avoided by running on the dev machines that are available outside notebooks, I think it's a good lesson for how the Rubin data is structured and how we can more efficiently import with our existing tools.\n",
    "\n",
    "Useful material:\n",
    "- LINCC notebooks: https://github.com/lsst-sitcom/linccf\n",
    "- https://github.com/LSSTScienceCollaborations/StackClub/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab38dd4-ec6d-4d45-9b0d-f41652e9779b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:42:39.182907Z",
     "iopub.status.busy": "2025-01-30T17:42:39.182565Z",
     "iopub.status.idle": "2025-01-30T17:42:41.751515Z",
     "shell.execute_reply": "2025-01-30T17:42:41.751053Z",
     "shell.execute_reply.started": "2025-01-30T17:42:39.182888Z"
    }
   },
   "outputs": [],
   "source": [
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15807dad",
   "metadata": {},
   "source": [
    "### Set DRP_VERSION and COLLECTION_TAG\n",
    "\n",
    "1. Update the `DRP_VERSION` and `COLLECTION_TAGS` in *00-set_env.sh*.\n",
    "2. Source the script: ```source 00-set_env.sh```.\n",
    "3. Run Jupyter from the same terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a6b07-f916-4e26-8862-ed50f2e669d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:42:37.892434Z",
     "iopub.status.busy": "2025-01-30T17:42:37.892134Z",
     "iopub.status.idle": "2025-01-30T17:42:37.895017Z",
     "shell.execute_reply": "2025-01-30T17:42:37.894645Z",
     "shell.execute_reply.started": "2025-01-30T17:42:37.892419Z"
    }
   },
   "outputs": [],
   "source": [
    "DRP_VERSION = os.environ[\"DRP_VERSION\"]\n",
    "COLLECTION_TAG = os.environ[\"COLLECTION_TAG\"]\n",
    "print(f\"DRP_VERSION: {DRP_VERSION}\")\n",
    "print(f\"COLLECTION_TAG: {COLLECTION_TAG}\")\n",
    "base_output_dir = Path(f\"/sdf/data/rubin/shared/lsdb_commissioning/hats/{DRP_VERSION}\")\n",
    "collections = f\"LSSTComCam/runs/DRP/DP1/{DRP_VERSION}/{COLLECTION_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d860e7b9-0230-41da-9d5f-aebcb925bb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:42:45.001874Z",
     "iopub.status.busy": "2025-01-30T17:42:45.001433Z",
     "iopub.status.idle": "2025-01-30T17:42:45.006916Z",
     "shell.execute_reply": "2025-01-30T17:42:45.006553Z",
     "shell.execute_reply.started": "2025-01-30T17:42:45.001858Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dir = base_output_dir / \"raw\"\n",
    "\n",
    "paths_dir = raw_dir / \"paths\"\n",
    "refs_dir = raw_dir / \"refs\"\n",
    "sizes_dir = raw_dir / \"sizes\"\n",
    "\n",
    "paths_dir.mkdir(parents=True, exist_ok=True)\n",
    "refs_dir.mkdir(parents=True, exist_ok=True)\n",
    "sizes_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59eeb7e-9f1f-45ef-99c7-6b8b6c1a398d",
   "metadata": {},
   "source": [
    "### Configure Butler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1a52d8-29d2-46f9-874d-239050c25cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:42:42.886979Z",
     "iopub.status.busy": "2025-01-30T17:42:42.886298Z",
     "iopub.status.idle": "2025-01-30T17:42:43.257163Z",
     "shell.execute_reply": "2025-01-30T17:42:43.256667Z",
     "shell.execute_reply.started": "2025-01-30T17:42:42.886961Z"
    }
   },
   "outputs": [],
   "source": [
    "config = '/repo/main'\n",
    "butler = dafButler.Butler(config, collections=collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1ed0e-7ce5-4f62-8683-204cddce9281",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70d1a98-e5ed-4c05-a279-1c7ac4e99775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:42:50.236168Z",
     "iopub.status.busy": "2025-01-30T17:42:50.235868Z",
     "iopub.status.idle": "2025-01-30T17:42:50.240800Z",
     "shell.execute_reply": "2025-01-30T17:42:50.240405Z",
     "shell.execute_reply.started": "2025-01-30T17:42:50.236150Z"
    }
   },
   "outputs": [],
   "source": [
    "def uris_from_butler(dataset_type, out_dir):\n",
    "    refs = butler.query_datasets(dataset_type)\n",
    "    paths = []\n",
    "    for _, ref in enumerate(tqdm(refs)):\n",
    "        table_path = butler.getURI(dataset_type, dataId=ref.dataId)\n",
    "        paths.append(table_path.path)\n",
    "    \n",
    "    print(f\"Found {len(paths)} files for {dataset_type}\")\n",
    "\n",
    "    file_pointer = out_dir / \"paths\" / f\"{dataset_type}.txt\"\n",
    "    with file_pointer.open(\"w\", encoding=\"utf8\") as _file:\n",
    "        for path in paths:\n",
    "            _file.write(path + \"\\n\")\n",
    "\n",
    "    ref_ids = [ref.dataId.mapping for ref in refs]\n",
    "    ref_frame = pd.DataFrame(ref_ids)\n",
    "    ref_frame.to_csv(out_dir / \"refs\" / f\"{dataset_type}.csv\", index=False)            \n",
    "    \n",
    "def download_visits(out_dir):\n",
    "    \"\"\"Downloads the visitTable for LSSTComCam\"\"\"\n",
    "    visits = butler.get(\"visitTable\", dataId={'instrument': 'LSSTComCam'})\n",
    "    parquet_path = out_dir / \"visits.parquet\"\n",
    "    visits.to_parquet(parquet_path)\n",
    "    print(f\"Saved {len(visits)} visits rows to {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2519023-4a48-481b-aa6a-3130524376c0",
   "metadata": {},
   "source": [
    "## Fetch all URIs.\n",
    "\n",
    "We write the file paths to a simple text file.\n",
    "\n",
    "Example outputs, to give an idea of number of files and total runtime:\n",
    "\n",
    "```\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 20.95it/s]\n",
    "Found 28 files for diaObjectTable_tract\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 22.46it/s]\n",
    "Found 28 files for diaSourceTable_tract\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [00:27<00:00, 21.54it/s]\n",
    "Found 586 files for forcedSourceOnDiaObjectTable\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 605/605 [00:27<00:00, 21.63it/s]\n",
    "Found 605 files for objectTable\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 16471/16471 [13:36<00:00, 20.16it/s]\n",
    "Found 16471 files for sourceTable\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 605/605 [00:28<00:00, 21.34it/s]\n",
    "Found 605 files for forcedSourceTable\n",
    "```\n",
    "\n",
    "This took a really long time, relative to what I expected, and I'll comment out the invocations.\n",
    "\n",
    "### CONCERN TO DM\n",
    "\n",
    "I'm concerned about the growth of the `sourceTable` in particular. This is already at `16_471` datasets. The result size of `butler.query_datasets(\"sourceTable\")` will soon be too large to handle, and there doesn't appear to be a mechanism in the existing API for pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b642-2666-4165-8c65-2bc382fdac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uris_from_butler('diaObjectTable_tract', raw_dir)\n",
    "uris_from_butler('diaSourceTable_tract', raw_dir)\n",
    "uris_from_butler('forcedSourceOnDiaObjectTable', raw_dir)\n",
    "uris_from_butler('objectTable', raw_dir)\n",
    "uris_from_butler('sourceTable', raw_dir)\n",
    "uris_from_butler('forcedSourceTable', raw_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a70cac",
   "metadata": {},
   "source": [
    "## Download visits table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae463ab-2e2e-4aec-8e2e-4bcb21a8c3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T14:48:17.090895Z",
     "iopub.status.busy": "2025-01-30T14:48:17.090607Z",
     "iopub.status.idle": "2025-01-30T14:48:17.472416Z",
     "shell.execute_reply": "2025-01-30T14:48:17.472035Z",
     "shell.execute_reply.started": "2025-01-30T14:48:17.090881Z"
    }
   },
   "outputs": [],
   "source": [
    "download_visits(raw_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
