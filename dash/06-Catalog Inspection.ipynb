{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5060a38-77e8-44d6-8e0d-877edaa7268b",
   "metadata": {},
   "source": [
    "# Catalog inspection\n",
    "\n",
    "I want to look at the \"fresh\" datasets (that have come directly from butler), and will also look at the post-processed DASH datasets (once I've generated them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e490de7-923f-4870-83b1-302075eabfb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:29:37.636251Z",
     "iopub.status.busy": "2025-01-30T16:29:37.636012Z",
     "iopub.status.idle": "2025-01-30T16:29:39.511734Z",
     "shell.execute_reply": "2025-01-30T16:29:39.511309Z",
     "shell.execute_reply.started": "2025-01-30T16:29:37.636235Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02c256-f540-462d-90ce-50e1f02f193b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:29:40.491018Z",
     "iopub.status.busy": "2025-01-30T16:29:40.490470Z",
     "iopub.status.idle": "2025-01-30T16:29:40.493475Z",
     "shell.execute_reply": "2025-01-30T16:29:40.493104Z",
     "shell.execute_reply.started": "2025-01-30T16:29:40.491001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRP_VERSION: w_2025_07\n"
     ]
    }
   ],
   "source": [
    "DRP_VERSION = os.environ[\"DRP_VERSION\"]\n",
    "print(f\"DRP_VERSION: {DRP_VERSION}\")\n",
    "base_output_dir = Path(f\"/sdf/data/rubin/shared/lsdb_commissioning\")\n",
    "raw_dir = base_output_dir / \"raw\" / DRP_VERSION\n",
    "hats_dir = base_output_dir / \"hats\" / DRP_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_dir = raw_dir / \"field_sizes\"\n",
    "fields_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50174cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_statistics(catalog_name):\n",
    "    parquet_file = pq.ParquetFile(hats_dir / catalog_name / \"dataset\" / \"_metadata\")\n",
    "\n",
    "    num_cols = parquet_file.metadata.num_columns\n",
    "    num_row_groups = parquet_file.metadata.num_row_groups\n",
    "    sizes = np.zeros(num_cols)\n",
    "\n",
    "    for rg in range(num_row_groups):\n",
    "        for col in range(num_cols):\n",
    "            sizes[col] += (\n",
    "                parquet_file.metadata.row_group(rg).column(col).total_compressed_size\n",
    "            )\n",
    "\n",
    "    ## This is just an attempt at pretty formatting\n",
    "    percents = [f\"{s/sizes.sum()*100:.1f}\" for s in sizes]\n",
    "\n",
    "    statistics = {\n",
    "        \"name\": parquet_file.schema.names,\n",
    "        \"size\": sizes.astype(int),\n",
    "        \"percent\": percents,\n",
    "    }\n",
    "    outfile = raw_dir / \"field_sizes\" / f\"{catalog_name}.csv\"\n",
    "    pd.DataFrame(statistics).sort_values(\"size\", ascending=False).to_csv(\n",
    "        outfile, index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19828-d3f3-4c9d-802d-b5a1c53d34a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T16:47:19.876780Z",
     "iopub.status.busy": "2025-01-30T16:47:19.876454Z",
     "iopub.status.idle": "2025-01-30T16:47:23.558146Z",
     "shell.execute_reply": "2025-01-30T16:47:23.557660Z",
     "shell.execute_reply.started": "2025-01-30T16:47:19.876757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diaObject\n",
      "num partitions 4\n",
      "num rows 1914697\n",
      "diaSource\n",
      "num partitions 4\n",
      "num rows 3593618\n",
      "diaForcedSource\n",
      "num partitions 225\n",
      "num rows 426871848\n",
      "object\n",
      "num partitions 54\n",
      "num rows 5439433\n",
      "source\n",
      "num partitions 117\n",
      "num rows 43826352\n",
      "forcedSource\n",
      "num partitions 196\n",
      "num rows 579119668\n"
     ]
    }
   ],
   "source": [
    "fresh_catalogs = [\n",
    "    \"diaObject\",\n",
    "    \"diaSource\",\n",
    "    \"diaForcedSource\",\n",
    "    \"object\",\n",
    "    \"source\",\n",
    "    \"forcedSource\",\n",
    "]\n",
    "\n",
    "for catalog_name in fresh_catalogs:\n",
    "    cat = hats.read_hats(hats_dir / catalog_name)\n",
    "    print(catalog_name)\n",
    "    print(\"num partitions\", len(cat.get_healpix_pixels()))\n",
    "    print(\"num rows\", cat.catalog_info.total_rows)\n",
    "    collect_statistics(catalog_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
